# run dataPreperation.py, fullEvaluation.py, and simulatePapers.py first

experiment_path = "./data/papers/"
full_evaluation_path = "./output/papers/"

# baseline 1: use only papers with relevant metrics (f1 score)

# baseline 2: use top 3 algo from each paper

# test 1: use the elo ranking to select the best models

# gold standard: use the full evaluation to select the best models